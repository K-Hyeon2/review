# 1111 화

</br>
</br>
</br>

#### 과대적합 (Overfitting)

>Train > Valid

- 훈련 데이터에 지나치게 맞춰져 학습 된 상태
- 새로운 데이터에서 성능이 낮게된다
- 데이터 양에 비해 복잡한 모델에서 발생
- 모델의 복잡도가 높다라고 한다

</br>

#### 과대적합 원인 및 해결방안

- 데이터 양 늘리기: 시간과 돈이 들기 때문에 현실적으로 어렵다
- 모델을 좀 더 단순하게 만들기: 사용한 모델보다 복잡도가 낮은 모델 사용


</br>
</br>


#### 과소적합 (Underfitting)

>Train < Valid

- 모델이 훈련 데이터를 제대로 학습하지 못한 상태
- 복잡한 패턴을 충분히 학습하지 못하여 발생
- 훈련 데이터와 새로운 데이터 모두 성능이 낮을 시 발생
- 모델의 복잡도가 낮다라고 한다

</br>

#### 과소적합 원인 및 해결방안

- 좀 더 복잡한 모델 사용
- 모델이 제공하는 규제 하이퍼 파라미터 조절

</br>
</br>
</br>


## 파이프라인 (Pipeline)

>일련 작업이나 프로세스가 순차적으로 연결되어 진행되는 구조


</br>
</br>
</br>


## SVM (Support Vector Machine)

</br>

>딥러닝 이전에 분류에서 뛰어난 성능으로 많이 사용되었던 분류 모델

#### Support Vector

> 양 클래스 간 가장 가까이 있는 값

#### Margine

> 두 Support Vector 간의 너비


#### 결정경계

>분류 문제에서 클래스들을 구분/분리 기준
>Train Dataset을 이용해 결정경계를 찾는다


</br>
</br>


#### Hard Margin

>Outlier(이상치)를 무시하지 않고 찾는다. 그래서 Support Vector 간의 거리(Margin)가 매우 좁아질 수 있다
>선형적으로 분리 가능할 때 작동하지만 그러지 않을 경우 Overfitting 발생

</br>
</br>

#### Soft Margin

>일부 Outlier(이상치)를 무시하고 찾는다. 일부 데이터 포인터가 결정경계를 침범하는 것을 허용
>무시 비율 하이퍼 파라미터 C로 정한다
>무시가 너무 커지면 Underfitting 문제 발생

</br>
</br>

#### Hard/Soft 설정 하이퍼 파라미터 C

- SVM의 규제 하이퍼 파라미터
- 노이즈가 있는데 선형적으로 분리되지 않을 때 C값을 조정해 마진 변경
- 기본값은 1
- 값이 클 수록 무시비율 낮아짐. 너무 크면 Overfitting 발생
- 값이 작을 수록 무시비율 높아짐. 너무 작으면 Underfitting 발생
- Overfitting 발생하면 값을 작게 설정
- Underfitting 발생하면 값을 크게 설정


</br>
</br>

#### Gamma

>개별 데이터 포인트가 결정 결계를 만드는데 어느 정도 영향력을 주는지 설정하는 값

- **비선형 결정경계**를 얼마나 유연하게 만들지 조절하는 규제 하이퍼 파라미터
- Overfitting이 발생하면 값을 작게 설정
- Underfitting이 발생하면 값을 크게 설정


</br>
</br>
</br>


<code>
<pre>

#SVM 모델링
#연속형:    Feature Scaling
#범주형:    One Hot Encoding


#데이터 불러오기
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

#데이터 나누기
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)

#데잍 변환
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)



#SVR: 회귀, SVC: 분류
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

#Linear SVM - 규제 hyper parameter: C
##작을 수록 규제 강도가 큼.
C_list = [0.001, 0.01, 0.1, 1, 10, 100] # 0 초과의 값을 지정. 실수. default: 1
train_acc_list = []
test_acc_list = []

for C in C_list:
    svm = SVC(
        kernel="linear", # 커널 함수 지정. 선형SVM: linear, 비선형SVM: rbf(기본), poly, sigmoid
        C=C,             # soft - hard margin 설정. (작을수록 강한 규제)
        random_state=0
    )
    # 학습
    svm.fit(X_train_scaled, y_train)
    # 검증
    ## 추론
    pred_train = svm.predict(X_train_scaled)
    pred_test = svm.predict(X_test_scaled)
    ## 평가
    train_acc_list.append(accuracy_score(y_train, pred_train))
    test_acc_list.append(accuracy_score(y_test, pred_test))




import pandas as pd
import numpy as np
df = pd.DataFrame({
    "C":np.log10(C_list),
    # "C": C_list,
    "Train": train_acc_list,
    "Test": test_acc_list
})
df.set_index("C")

</pre>
</code>



</br>
</br>
</br>




<code>
<pre>

###############################################################################
#비선형 SVM. Hyper Parameter - C: soft/hard margin 규제, gamma (기본: 1)

#gamma  변경에 따른 성능 변화.
###############################################################################
gamma_list = [0.001, 0.01, 0.1, 1, 5, 10, 100]
train_acc_list = []
test_acc_list = []
for gamma in gamma_list:
    svm = SVC(kernel="rbf", C=1, gamma=gamma)  # kernel기본값: rbf
    svm.fit(X_train_scaled, y_train)
    train_acc_list.append(accuracy_score(y_train, svm.predict(X_train_scaled)))
    test_acc_list.append(accuracy_score(y_test, svm.predict(X_test_scaled)))




df = pd.DataFrame({
    "gamma":np.log10(gamma_list),
    "Train":train_acc_list,
    "Test":test_acc_list
})

</pre>
</code>


</br>
</br>
</br>


## K-최근접 이웃

>분류와 회귀를 모두 지원한다
>학습은 빠르지만 예측 시간이 오래 걸린다

</br>

- 분류: 추론할 Feature들과 가까운 Feature들로 구성된 Data Point K개의 y중 다수의 class로 추론
- 회귀: 추론할 Feature들과 가까운 Feature들로 구성된 Data Point K개의 y값의 평균값으로 추론
- K가 너무 적으면 Overfitting, 너무 크면 Underfitting 발생

</br>
</br>
</br>


## 의사결정나무 (Decision Tree)

</br>

>스무고개와 유사한 형식의 알고리즘

- 분류: 불순도를 가장 낮출 수 있는 조건을 찾아 분기한다
- 회귀: 오차가 가장 적은 조건을 찾아 분기한다
- 머신러닝 모델 중에서도 해석이 가능한 몇 안되는 White-Box 모델
- 과대적합이 발생하기 쉬운 특성이 있다

</br>
</br>

>순도(Purity) / 불순도(Impurity) 의미
>- 서로 다른 종류의 값이 섞여 있는 비율
>- 특정 클래스의 값이 많을수록 순도가 높다

</br>

- White Box:    모델이 추정한 결과의 이유를 확인할 수 있는 모델
- Black Box:    결과의 이유를 확인 할 수 없는 모델

</br>
</br>
</br>

#### DecisionTree에서 Overfitting 문제

- 분류: 모든 데이터셋이 불순도가 0이 될 때 까지 분기
- 회귀: MSE가 0이 돌 때 까지 분기
- 하위 노드가 더 이상 생성되지 않도록 하는 것은 가지치기라고 함

</br>

>max_depth
- 트리의 최대 깊이(질문 단계)를 정의
- 기본값: None 깊이 제한 없이 완벽히 분할 될 때 까지 분기한다
- 분류:  불순도가 0이 될 때 까지
- 회귀:  MSE가 0이 될 때 까지

</br>

>max_leaf_node
- Leaf Node 개수를 제한한다
- 기본값:   None - 제한 없다

</br>

>min_samples_leaf
- Leaf Node가 가져야 하는 최소한의 sample 수를 지정
- 기본값:   1 -> 제한이 없다


</br>
</br>

#### Wine Dataset을 이용한 Color 분류

<code>
<pre>
##데이터셋 로드
import pandas as pd
wine = pd.read_csv("data/wine.csv")



##X, y 분리
X = wine.drop(columns='color').values
y = wine['color'].values



##Train / Test set 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)



##라벨 인코딩
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(['A', 'B', 'C'])
X_train[:, -1] = le.transform(X_train[:, -1]) # quality를 조회 변환.
X_test[:, -1] = le.transform(X_test[:, -1])



##DecisionTreeClassifier 생성, 학습, 검증
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
</pre>
</code>


</br>
</br>
</br>


#### Ensemble (앙상블)

- 여러 모델을 학습시켜 결합하는 방식
- 개별로 학습한 여러 모델을 조합하여 과적합을 막고 일반화 성능 향상 기대
- 개별 모델의 성능이 확보되지 않을 때 성능향상 기대

</br>

>앙상블의 종류
- Bagging:  같은 유형의 알고리즘들을 조합하되 각각 합습하는 데이터를 다르게 한다
- voting:   서로 다른 종류의 알고리즘들을 결합한다

</br>

>부스팅
- 약한 학습기들을 결합해서 강력한 학습기로 만든다
- 각 약한 학습기들은 순서대로 일 하며 뒤의 학습기들은 앞의 학습기가 찾기 못한 부분을 추가로 찾는다





</br>
</br>
</br>




#### Random Forest (랜덤 포레스트)

- Bagging 방식의 앙상블 모델
- Decision Tree를 기반으로 한다
- 다수의 Decision Tree를 사용한다
- 처리속도가 빠르며 성능도 높은 모델이다


</br>

<code>
<pre>


##데이터셋 로드 및 Train / Test set 분리##
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('data/wine.csv')
X = df.drop(columns=['color', 'quality'])
y = df['color']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)



##RandomForestClassifier 생성, 학습, 검증##
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(
    n_estimators=200, # DecisionTree 개수. (최소 200개)
    max_features=10,  # 지정한 feature수 내에서 random하게 feature들을 선택.
    max_depth=5,      # DecisionTree hyper parameter (모든 Decision Tree 모델들은 동일한 하이퍼파라미터를 가진다..)
    random_state=0,
    n_jobs=-1,        # 개별 DecisionTree 학습, 추론시 병렬 처리 할 때 사용할 프로세서 개수.(각 모델은 독립적으로 학습/추정한다. -1 : 모든 프로세서 다 사용)
)





##학습(Train)##
rfc.fit(X_train, y_train)

##검증##
##추론: 클래스 결과과##
pred_train = rfc.predict(X_train)
pred_test = rfc.predict(X_test)

##추론: 클래스별 확률 결과##
pred_train_proba = rfc.predict_proba(X_train)
pred_test_proba = rfc.predict_proba(X_test)
</pre>
</code>