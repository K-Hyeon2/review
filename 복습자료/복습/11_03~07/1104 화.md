# 1104 화

</br>
</br>
</br>

## 배열 연산

>배열간의 연산은 같은 inx의 원소끼리 계산한다
- 원소별 연산이라고도 함
- 배열간 연산시 shape가 같아야 함
- 배열이 다를 경우 브로드 캐스트 조건을 만족하면 연산 가능


</br>
</br>

#### 내적(Dot product) 연산

>"@" 연산자 또는 numpy.dot 함수 사용

- 같은 inx 원소끼리 곱한 뒤 모두 더한다
- 백터간의 내적 결과는 스칼라가 됨
- 가중합을 구할 때 주로 사용함
- 앞(행벡터) / 뒤(열백터)

</br>

<code>
<pre>

x = np.array([1, 2, 3])
y = np.array([4, 5, 6])
z = np.sum(x * y)


a = x * y

print(z)        #32
print(a)        #[4  10  18]


x.T @ y         #32
np.dot(x, y)    #32

</pre>
</code>

D

#### 행렬 곱

- 같은 inx의 앞 행렬의 행과 뒤 행렬의 열간 내적한다
- 앞 행렬의 열수와 뒤 행렬의 행수가 같아야 한다
- (2, 3) @ (2, 3)   #계산 안됨
- (2, 3) @ (3, 2)   #계산 가능


<code>
<pre>

A = np.arange(1, 7).reshape(2, 3)
B = np.arange(1, 7).reshape(3, 2)
C = A.copy()

A @ B       #([[22, 28], [49,64]])
A @ C       #계산 안됨




#4명이 산 과일 개수 (가중치 행)
cnts = np.array([
    [10, 20, 2], 
    [5, 15, 1],
    [100, 200, 20], 
    [30, 20, 10]
])
price = price[..., np.newaxis]
cnts.shape, price.shape
#((4, 3), (3, 1))

cnts @ price
#array([[60000],
        [35000],
        [600000],
        [180000]])
#한 번에 구할 수 있음

</pre>
</code>

</br>

#### 기술통계함수


</br>
</br>
</br>

## 브로드 캐스팅

>형태가 다른 배열 연산시 배열의 형태를 맞추어 연산 가능하도록 한다

- 두 배열의 축의 개수가 다르면 작은 축의 개수 형태의 앞에 1을 채운다
- 각 차원의 크기가 다른 경우 어느 하나 1이 있으면 1이 다른 배열의 크기와 일치하도록 늘어난다

</br>

(3, 2) + (3, 1)         #가능
(3, 2) + (3, 3)         #불가능
(3, 1, 5) + (1, 7, 1)   #가능
(3, 1, 5) + (1, 7, 2)   #불가능
(3, 4, 5) + (1, 1, 5)   #가능


</br>
</br>
</br>



## 머신러닝의 HelloWorld

</br>

#### scikit-learn 내장 데이터셋 가져오기

>머신러닝 모델을 테스트 하기 위한 데이터셋을 제공한다
- 패키지:   sklearn.datasets
- 함수:     load_xxxx()

</br>

#### scikit-learn 내장 데이터셋 구성

>scikit-learn의 dataset은 딕셔너리 구조의 Bunch 클래스 객체

- target_names: 예측하려는 값(class)을 가진 문자열 배열
- target:       Label(출력데이터)
- data:         Feature(입력변수)
- feature_names:    입력변수 각 항목의 이름
- DBSCR:        데이터셋에 대한 설명


<code>
<pre>

from sklearn.datasets import load_iris
iris = load_iris()  #내장 데이터셋 가져오기


iris.keys()
#Dataset 구성 key값들 조회
#딕셔너리 형태로 결과 출력


iris['data'].shape
#입력변수 조회
#내장 데이터 iris (150, 4)


iris['feature_names']
#입력변수명 조회
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']


iris['target']
#문자열이 아닌 숫자로 반환이 되는데
#x와 y값 연산을 위하여 숫자로 반환된다


iris['target_names']
#출력 변수의 class의 의미 조회
#array(['setosa', 'versicolor', 'virginica'], dtype='<U10')

</pre>
</code>


</br>
</br>
</br>


#### Iris 데이터 셋을 판다스 데이터 프레임으로 구성

</br>


<code>
<pre>

import pandas as pd         #pandas 연결

df = pd.DataFrame(
    iris['data'], 
    columns=iris['feature_names']
)                           #데이터 프레임 생성
df['품종'] = iris['target'] #'품종'을 열로 추가
df.head()                   #idx 5번까지 출력

df['품종2'] = df['품종'].apply(lambda i : iris['target_names'][i])
#'품종2'의 열을 추가


</pre>
</code>



</br>
</br>
</br>


## 머신러닝 적용

>프로그래머가 직접 패턴을 만드는 대신 컴퓨터가 데이터를 학습하여 규칙을 자동으로 만드는 

#### 결정트리 알고리즘을 이용한 분ㅂ류
>독립 변수의 조건에 따라 종속 변수를 분리

- import 모델
- 모델 생성
- 모델 학습시키기
- 예측


</br>
</br>



<code>
<pre>
from sklearn.tree import DecisionTreeClassifier
#1. import 모델
#xxxxxClassifier: 분류 모델
#xxxxxRegressor : 회귀
# 범주형 모델 (값이 정해져 있음)


model = DecisionTreeClassifier()
#2. 모델 생성
#model변수에 모델 값 넣기


model.fit(iris['data'], iris['target'])
#3. 모델 학습시키기
#물음표 아이콘 옆에 "fitted" 문구가 보이면 학습 완료


import numpy as np

new_data = np.array([
    [5, 3.5, 1.4, 0.25], 
    [2, 2.2, 5.3, 2.2], 
    [1.2, 5, 3.2, 7.6]
])
print(new_data.shape)       
#(3, 4)

pred = model.predict(new_data)      #x를 입력해서 y를 예측
print(pred)
#[0 2 1] 정답 확인
print(iris['target_names'][pred])
#4. 예측
</pre>
</code>



>이번 학습을 정리하면서 이해가 되지 않는 부분이 너무 많았다. 계속 복습을 하여 이해가 될 때까지 할 것이다.