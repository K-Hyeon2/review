# 1105 수

</br>
</br>
</br>

## 데이터셋 (Dataset)

#### Train (훈련/학습 데이터셋)
>공부하는 데이터, 모델 학습할 때 사용

</br>

#### Validation (검증 데이터셋)
>하이퍼 파라미터 튜닝시 성능 검증을 위해 사용
>모델의 하이퍼 파라미터를 변경하여 성능 향상 작업
>평가 하기 전 모의고사 같은 작업

</br>

#### Test (평가 데이터셋)
>모델 성능 최종 측정
>딱 한 번만 사용되어야 한다

</br>

#### 성능이 마음에 안들 때

- Train:        모델 수정 및 데이터 전처리 작업
- Validatuion:  하이퍼 파라미터 변경
- Test:         최종 테스트

</br>
</br>
</br>

## Data 분리 방식

</br>

#### Hold Out

>데이터셋을 Train, Validation, Test set으로 나눈다
>sklearn.model_selection.train_test_split() 함수 사용

</br>

<code>
<pre>

from sklearn.datasets import load_iris
#load_xxxxx()


X, y = load_iris(return_X_y=True)
#iris.data와 iris.target값만 추출.  (X, y)


from sklearn.model_selection import train_test_split

##train_test_split():    train, test 2개로 1차 분리##
X_train, X_test, y_train, y_test = train_test_split(
    X, # input
    y, # output 
    test_size=0.2,      #testset의 비율. default: 0.25
    stratify=y,         #분류 데이터셋에만 적용. 
                         (y(target)이 범주형) 원본의 클래스들 비율과 동일한 비율로 나누기
    random_state=0      #random seed값
                         나누기 전에 shuffle(섞기)을 먼저 하는 데 그때 일정하게 섞이게 하기 위해 seed값 지정.
)



##train_test_split():    train, val 2개로 2차 분리##
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, 
    test_size=0.2,
    stratify=y_train,   # stratify=output(y값, target, label) 지정.
    random_state=0)

</pre>
</code>

</br>

>위 코드와 같이 train_test_split()을 2번 사용하여 Train, Validation, Test 셋으로 분리

</br>
</br>
</br>

<code>
<pre>

# 모델 생성
#max_depth: DecisionTree의 하이퍼 파라미터 중 하나.
#max_depth = 1
#max_depth = 2
#max_depth = 3
#max_depth = 4  
tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)

#모델 학습 - trainset
tree.fit(X_train, y_train)

#모델 검증 - validation set /train set
#1. 예측(추론)
pred_train = tree.predict(X_train)
pred_val = tree.predict(X_val)

#2. 평가(검증) ==> 정확도
train_acc = accuracy_score(y_train, pred_train)
val_acc = accuracy_score(y_val, pred_val)



#max_depth(하이퍼파라미터) 별 평가 결과
print(f"max_depth: {max_depth}")        #사용한 max_depth 값
print("Train accuracy:", train_acc)     #Train 정확도 추출
print("Validation accuracy:", val_acc)  #Val 정확도 추출

###만약 Train: 1.0, Val: 0.95 일 시 Train에 과대적합이 되어 있으므로
        Train: 0.95, Val 1.0 일 때가 더 좋은 모델이라고 할 수 있다

##따라서 적당한 max_depth를 입력하여 가장 좋은 성능을 찾는 것이다.


max_depth = 4
#validation에서 가장 성능 좋은 하이퍼파라미터 사용.

model = DecisionTreeClassifier(max_depth=max_depth, random_state=0)
model.fit(X_train, y_train)             #모델 교육
pred = model.predict(X_test)            #모델 예측
test_acc = accuracy_score(y_test, pred) #정확도 추출
print("최종 평가결과:", test_acc)        #최종 평가결과: 0.966...

</pre>
</code>


</br>
</br>
</br>

#### Hold out 단점

>Train, Validation, Test 어떻게 나누는지에 따라 결과가 다름

- 이상치에 대한 영향을 많이 받음
- 데이터 양이 많아야 함
- 새로운 데이터에 대한 성능이 떨어짐


</br>
</br>


#### K-겹 교차검층 (K-Fold Cross Validation)

- 데이터셋을 설정한 K개로 나눈다
- K개 중 하나를 훈련 세트로 하여 학습
- K개 모두가 한 번씩 학습 후 평가지표를 평균내어 모델 성능 평가
- 데이터양이 충분하지 않을 때 사용
- y값이 회귀일 때:  KFold
- y값이 분류일 때:  StratifiedKFold

</br>
</br>


<code>
<pre>

from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error # 오차제곱 평균.(회귀 평가지표중 하나.)
import numpy as np

###dataset: X, y (위에서 조회한 값 사용)
mse_list = [] # iteration 별 검증 결과를 저장할 리스트
kfold = KFold(n_splits=5)
gen = kfold.split(X) # generator는 index들을 제공


for train_idx, test_idx in gen:
    #tuple(trainset index: ndarray,  testset index: ndarray)
    #X, y에서 조회한 idx를 이용해 Train/Test 셋을 생성
    X_train, y_train = X[train_idx], y[train_idx]
    X_test, y_test = X[test_idx], y[test_idx]

    # 모델 생성
    model = DecisionTreeRegressor(max_depth=2, random_state=0)
    # 학습
    model.fit(X_train, y_train)
    # 검증
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test, pred)
    #mean_squared_error: 회귀의 평가지표 np.mean((정답 - 추정값값)**2)
    mse_list.append(mse)

</pre>
</code>


</br>
</br>
</br>


#### cross_val_score()

>교차검증을 처리하는 함수
- 데이터셋을 K개로 나누고 K번 반복하면서 평가하는 함수
- 평가 지표를 하나만 사용

</br>

>주요매개변수
- estimator:    모델객체
- X:            Feature
- y:            Label
- scoring:      평가함수, 함수객체
- cv:           나눌 개수(K)
- 반환값:       array(각 반복마다의 평가점수)

</br>

<code>
<pre>

y = df['MEDV'].values
X = df.drop(columns="MEDV").values
X.shape #(506, 13)
y.shape #(506, )


####모델링 - 하이퍼파라미터 튜닝을 통해서 가장 성능 좋은 모델을 찾기
##하이퍼파라미터 - max_depth

max_depth_list = [1, 2, 3, 4, 5]
#max_depth별 모델의 검증 결과를 저장할 딕셔너리. key: max_depth, scores, mean_score

results = {"max_depth":[], "scores":[], "mean_score":[]}  

for max_depth in max_depth_list:
    model = DecisionTreeRegressor(max_depth=max_depth, random_state=0)
    # 교차검증을 이용해 성능 평가.
    scores = cross_val_score(
        estimator=model,
        X=X_train, 
        y=y_train,
        scoring="neg_mean_squared_error", 
        cv=4
    )
    # 결과 dictionary에 저장
    results['max_depth'].append(max_depth)
    results['scores'].append(scores)
    results['mean_score'].append(np.mean(scores))

</pre>
</code>



</br>
</br>
</br>



#### cross_validate()

>교차검증을 처리하는 함수
- 데이터셋을 K개로 나누고 K번 반복 평가 작업 처리 함수
- 평가 지표를 **여러 개** 사용
- 주요매개변수는 cross_val_score()와 같음


</br>
</br>
</br>


## Data 전처리

>모델링 하기 전 일이며 적합한 형태로 데이터셋을 변환 또는 조정하는 과정

</br>

>Garbage in, Garbage out
- 좋은 Dataset으로 학습해야 좋은 모델을 만들 수 있다
- 좋은 Train dataset을 만드는 것이 모델 성능에 가장 큰 영향을 준다

</br>

>Data Cleaning (데이터 정제)
- 데이터셋에 있는 오류, 불필요, 결측치, 중복 값 등을 제거 작업

</br>

>컬럼 선택 및 파생변수 생성
- 컬럼들 중 분석에 필요한 컬럼만 선택 및 기존 컬럼들을 계산한 결과값을 가지는 파생변수를 생성

</br>

>Feature의 데이터 타입 별 변환
- 문자열 - 날짜 타입으로 변환
- 범주형 - 수치형으로 변환
- 수치형 데이터 Feature Scaling
- 범주형 데이터 인코딩


</br>
</br>
</br>


#### 결측치(Missing Value) 처리

>결측치를 모델링 전 데이터 전처리 과정에서 처리해야함

</br>

>결측치 처리 방법

- 처리하기 전에 **누락된 것인지 존재하지 않는 것인지** 확인 해야함
- 존재하지 않은 것: 결측치 유지
- 누락된 결측치:    어떤 값인지 추측

</br>

>결측치 삭제

- 리스트 와이즈 삭제(Listwise Deletion) 행 삭제
- 컬럼 삭제 (Drop Column) 열 삭제


<code>
<pre>

#예
import pandas as pd
import numpy as np
data = {
    "name":['김영희', '이명수', '박진우', '이수영', '오영미'],
    "age": [23, 18, 25, 32, np.nan], 
    "weight":[np.nan, 80, np.nan, 57, 48]
}
df = pd.DataFrame(data)





#결측치 값 확

print(pd.isna(None))        #True
print(pd.isna(np.nan))      #True
print(pd.isna(pd.NA))       #True
print(np.nan == None)       #Fales
print(np.nan == np.nan)     #False


df.dropna()                 #리스트와즈, default, 행 단위 제거

df.dropna(axis=1)
#컬럼 단위, 열 단위 제거

</pre>
</code>


</br>
</br>
</br>


#### 결측치 대체(imputation)

>평균(mean)/중앙값(median)/최빈값(mode) 대체

</br>

- 평균: 수치형 컬럼, 정규 분포를 따르거나 극단치가 없는 경우 사용
- 중앙: 수치형 컬럼, 극단치 존재하거나 보통 평균보다 중앙값 사용
- 최빈: 범주형 컬럼의 경우 대표값인 최빈값 대체

</br>

>모델링 기반 대체 (K-NN)

- 결측치가 있는 컬럼을 Output(종속변수)로 결측치가 없는 행들(독립변수)을 Input하여 결측치 예측

</br>

>K-최근접 이웃 대체

- 결측치가 있는 데이터 포인트와 가장 가까운 K개의 데이터 포인트를 찾아 그 값들의 평균이나 최빈값으로 결측치 대





<code>
<pre>

df = pd.DataFrame([
        [0.1, 2.2, np.nan],
        [0.3, 4.1, 1], 
        [np.nan, 6, 1],
        [0.08, np.nan, 2],
        [0.12, 2.4, 1],
        [np.nan, 1.1, 3]
    ], columns=['A', 'B', 'C']
)


df['A']  = df['A'].fillna(df['A'].mean())
#A컬럼의 결측치를 평균으로 대체
#하기 전 극단치가 없는지 확인


df['B'] = df['B'].fillna(df['B'].median())
#B컬럼의 결측치를 중앙값으로 대체
#극단치가 있을 경우 중앙값 사용


df['C'] = df['C'].fillna(df['C'].mode())
#C컬럼의 결측치를 최빈값으로 대체
#범주형 컬럼인 경우 사용

</pre>
</code>



</br>
</br>
</br>



## scikit-learn 전처리기 이용한 대체

</br>
</br>

#### SimpleImputer

- 전처리 클래스로 평균/중앙값/최빈값으로 대체
- strategy: 어떤 값으로 대체할 지 지정
- mean:     평균값
- median:   중앙값
- most_frequent:    최빈값
- constant: 상수 (fill_value=채울값) 택 1

</br>

#### KNNImputer

- ML알고리즘을 이용해 결측치를 추정 및 대체
- 결측값이 있는 샘플의 최근접 이웃을 찾아 평균내어 결측값 대체
- fit:  변환할 때 필요한 값을 instance에 저장
- transform:        fit에서 찾은 값을 결측치 대체
- fit_transform:    fit - transform을 순서대로 한 번에 처리


<code>
<pre>
    #### SimpleImputer ####



df = org.copy()

from sklearn.impute import SimpleImputer
    #A, B (수치형) => 중앙값, C(범주형) => 최빈값
imputer1 = SimpleImputer(strategy="median")
imputer2 = SimpleImputer(strategy="most_frequent")

imputer1.fit(df[['A', 'B']])  # 결측치를 어떤 값으로 바꿀지 학습. (2차원 -> 0축 기준으로 계산)
result1 = imputer1.transform(df[['A', 'B']])  # 변환작업 (fit에서 찾은 중앙값으로 결측치를 대체)

result2 = imputer2.fit_transform(df['C'].to_frame()) #series.to_frame() : Series->DataFrame
    #fit/transform 을 순서대로 실행. fit/transform을 같은 데이터셋으로 할 경우 사용.

    #result1, result2 하나로 합치기.
    #ndarray 합치는 함수: np.concatenate([대상 배열들], axis=합칠방향(default: 0))
result = np.concatenate([result1, result2], axis=1)

</pre>
</code>

</br>
</br>
</br>


<code>
<pre>

    #### KNNImputer ####

df = org.copy()
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=3)
    #K - 가까운 데이터포인트 몇개를 확인 할지.
result = imputer.fit_transform(df)
    #imputer.fit(df) -> imputer.transform(df)

</pre>
</code>

>둘 중 성능이 좋은 전처리 방법을 택 1 하여 사용하면 된다



>전처리 방법은 이해가 됐지만, 데이터셋 나누기 중 KFold는 완벽히 이해하지 못했다. 복습을 통해 다음 시간 전까지 코드를 보며 이해할 것 이다.
